{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4885cd97-4ca0-4b39-8e82-a178f640696c",
   "metadata": {},
   "source": [
    "### 1. Ordinary Least Squares (OLS) Perspective\n",
    "\n",
    "#### 1.1 Model Representation:\n",
    "The linear regression model is represented as:\n",
    "$$\n",
    "\\hat{Y} = X\\theta\n",
    "$$\n",
    "where:\n",
    "- $ X $ is the design matrix with dimensions $ n \\times (m+1) $ (including the intercept term).\n",
    "- $ \\theta$ is the parameter vector with dimensions $ (m+1) \\times 1 $.\n",
    "\n",
    "#### 1.2 Loss Function:\n",
    "The mean squared error (MSE) is given by:\n",
    "- No regularization\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}(X\\theta - Y)^T(X\\theta - Y)\n",
    "$$\n",
    "- L1 regularization\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}(X\\theta - Y)^T(X\\theta - Y) + \\lambda \\|\\mathbf{\\theta}\\|_1\n",
    "$$\n",
    "- L2 regularization\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}(X\\theta - Y)^T(X\\theta - Y) + \\lambda\\theta^T\\theta\n",
    "$$\n",
    "\n",
    "#### 1.3 Objective:\n",
    "Given a dataset with $n$ samples and $m$ features, we want to find the parameter vector $\\theta$ that minimizes the mean squared error (MSE) between predicted values $\\hat{Y}$ and actual values $y$.\n",
    "$$\n",
    "\\theta = \\arg\\min_\\theta{J(\\theta)}\n",
    "$$\n",
    "\n",
    "#### 1.4. Solution:\n",
    "1. **Gradient of $\\theta$:**\n",
    "    - No regularization\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{2}{n}X^T(X\\theta - y)\n",
    "$$\n",
    "    - L1 regularization\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{2}{n}X^T(X\\theta - y)+\\lambda \\text{sign}(\\mathbf{\\theta})\n",
    "$$\n",
    "    - L2 regularization\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{2}{n}X^T(X\\theta - y)+2\\lambda\\theta\n",
    "$$\n",
    "2. **Closed-Form for $\\theta$:**\n",
    "    - No regularization\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\frac{2}{n}X^T(X\\theta - y) = 0 \\\\\n",
    "& X^TX\\theta - X^Ty = 0 \\\\\n",
    "& \\theta = (X^TX)^{-1}X^Ty\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "    - L1 regularization\n",
    "    \n",
    "    The closed-form solution for OLS with L1 penalty doesn't have a direct analytical solution due to the non-differentiability of the L1 norm. However, it can be solved using optimization algorithms like coordinate descent or proximal gradient descent. These algorithms iteratively update the coefficients until convergence.\n",
    "    \n",
    "    - L2 regularization\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\frac{2}{n}X^T(X\\theta - y)+2\\lambda\\theta = 0 \\\\\n",
    "& (X^TX+\\lambda n I)\\theta = X^Ty \\\\\n",
    "& \\theta = (X^T X + \\lambda n I)^{-1} X^Ty\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "### 2. Maximum Likelihood Estimation Perspective\n",
    "\n",
    "#### 2.1 Model Representation:\n",
    "The linear regression model is represented as:\n",
    "$$\n",
    "y^{(i)} = x^{(i)T}\\theta + \\epsilon\n",
    "$$\n",
    "where:\n",
    "- $x^{(i)}$ is the $i$th example which is a vector with dimensions $ (m+1) \\times 1 $. (including the intercept term).\n",
    "- $\\theta$ is the parameter vector with dimensions $ (m+1) \\times 1 $.\n",
    "- $\\epsilon $ is the error term assumed to be normally distributed with mean zero and variance $ \\sigma^2 $, that is  $\\epsilon\\sim{N(0,\\sigma^2)}$.\n",
    "\n",
    "#### 2.2 Likelihood Function:\n",
    "Under the assumption of Gaussian errors, the likelihood function of the observed data is given by the product of the probability density functions of the individual observations:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\theta | X, Y) & = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y^{(i)} - x^{(i)}\\theta)^2}{2\\sigma^2}\\right) \\\\\n",
    "& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left(-\\frac{\\sum_{i=1}^{n} (y^{(i)} - x^{(i)}\\theta)^2}{2\\sigma^2}\\right)} \\\\\n",
    "& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left(-\\frac{(X\\theta - Y)^T(X\\theta - Y)}{2\\sigma^2}\\right)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### 2.3 Objective:\n",
    "We seek to find the parameter vector $\\theta$ that maximizes the likelihood function of the observed data\n",
    "\n",
    "$$\n",
    "\\theta = \\arg\\max_\\theta{\\mathcal{L}(\\theta | X, Y)}=\\arg\\min_\\theta{(X\\theta - Y)^T(X\\theta - Y)}= \\arg\\min_\\theta{J(\\theta)}= (X^TX)^{-1}X^TY$$\n",
    "\n",
    "Although the object function is different, the form is the same with OLS.\n",
    "\n",
    "### 3. Column space Perspective\n",
    "![](https://pic1.zhimg.com/80/v2-437c1820680c560b2a7096977ce2e3a1_1440w.webp?source=1def8aca)\n",
    "If we take $X\\theta$ as a vector from the column space of $X$, then $X\\theta - Y$ is the residual vector, and $(X\\theta - Y)^T(X\\theta - Y)$ is the square of residual vector magnitude. So Minimizing MSE is equivalent to find the shortest residual vector which is orthogonal to the column space, that is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& X^T(X\\theta - Y) = 0 \\\\\n",
    "& X^TX\\theta =X^TY \\\\\n",
    "& \\theta = (X^TX)^{-1}X^TY\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b80f7b3-9532-41a6-82a0-b8538f910803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000, method='closed_form', reg=None, alpha=0.01):\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.epochs = epochs  # Number of iterations\n",
    "        self.method = method  # Method: 'closed_form' for closed-form solution, 'gradient_descent' for gradient descent\n",
    "        self.theta = None  # Parameters\n",
    "        self.reg = reg  # Regularization method: None (no penalty), 'l1' (L1 regularization), 'l2' (L2 regularization)\n",
    "        self.alpha = alpha  # Regularization parameter\n",
    "        self.loss_history = [] # Loss record\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add bias term\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        if self.method == 'closed_form':\n",
    "            # Closed-form solution\n",
    "            if self.reg is None:\n",
    "                # No regularization\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "            elif self.reg == 'l1':\n",
    "                # L1 regularization (Lasso)\n",
    "                L = np.eye(X_b.shape[1])\n",
    "                L[0, 0] = 0  # No penalty for the bias term\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b) + self.alpha * L).dot(X_b.T).dot(y)\n",
    "            elif self.reg == 'l2':\n",
    "                # L2 regularization (Ridge)\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b) + self.alpha * np.eye(X_b.shape[1])).dot(X_b.T).dot(y)\n",
    "        elif self.method == 'gradient_descent':\n",
    "            # Initialize parameters\n",
    "            self.theta = np.random.randn(X_b.shape[1], 1)\n",
    "            # Gradient descent\n",
    "            for _ in range(self.epochs):\n",
    "                gradients = 2 / len(X_b) * X_b.T.dot(X_b.dot(self.theta) - y)\n",
    "                if self.reg == 'l1':\n",
    "                    # L1 regularization (Lasso)\n",
    "                    gradients[1:] += self.alpha * np.sign(self.theta[1:])\n",
    "                elif self.reg == 'l2':\n",
    "                    # L2 regularization (Ridge)\n",
    "                    gradients[1:] += 2 * self.alpha * self.theta[1:]\n",
    "                self.theta -= self.lr * gradients\n",
    "                loss = np.mean((X_b.dot(self.theta) - y) ** 2)\n",
    "                self.loss_history.append(loss)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method. Choose 'closed_form' or 'gradient_descent'.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5361ec-405e-4f3b-8a3b-b0bfaaff2114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "# Introduce outliers\n",
    "# X = np.linspace(0,100,100)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_outliers = np.array([[1], [1.01], [1.02],[1.03], [1.04], [1.05]])\n",
    "y_outliers = np.array([[30], [35], [39],[40], [41], [42]])\n",
    "X = np.vstack((X, X_outliers))\n",
    "y = np.vstack((y, y_outliers))\n",
    "methods = ['closed_form','gradient_descent']\n",
    "regs = [None,'l1','l2']\n",
    "# Instantiate and fit the models\n",
    "# No penalty (ordinary linear regression)\n",
    "models = {}\n",
    "for m in methods:\n",
    "    for r in regs:\n",
    "        models['{}_{}'.format(m,r)] = LinearRegression(method=m,reg=r,alpha=10)\n",
    "        models['{}_{}'.format(m,r)].fit(X,y)\n",
    "w, b = [], []\n",
    "for k in models:\n",
    "    p = models[k].theta.ravel()\n",
    "    w.append(p[0])\n",
    "    b.append(p[1])\n",
    "df = pd.DataFrame({'solution':list(models.keys()),'w':w,'b':b})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a6252-6f75-47ec-90e4-0dc8e4d816dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize data and fitted models\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Scatter plot of original data\n",
    "plt.scatter(X, y, color='blue', label='Original data')\n",
    "\n",
    "# Fitted lines for closed-form solution and gradient descent\n",
    "for m in methods:\n",
    "    for r in regs:\n",
    "        plt.plot(X, models['{}_{}'.format(m,r)].predict(X), label='{}_{}'.format(m,r))\n",
    "# plt.plot(X, lin_reg_no_penalty_gd.predict(X), color='red', label='Gradient descent')\n",
    "\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4671845-8188-46d5-8b72-8dd530a0de23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot loss change during gradient descent\n",
    "plt.figure(figsize=(8, 5))\n",
    "for r in regs:\n",
    "    # plt.plot(X, models['{}_{}'.format(m,r)].predict(X), label='{}_{}'.format(m,r))\n",
    "    plt.plot(range(len(models['{}_{}'.format(methods[1],r)].loss_history)), \n",
    "             models['{}_{}'.format(methods[1],r)].loss_history, \n",
    "             label='{}_{}'.format(methods[1],r))\n",
    "plt.title('Gradient Descent Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
