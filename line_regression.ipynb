{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4885cd97-4ca0-4b39-8e82-a178f640696c",
   "metadata": {},
   "source": [
    "## Closed-Form Solution\n",
    "### 1. Ordinary Least Squares (OLS) Perspective\n",
    "\n",
    "#### 1.1 Model Representation:\n",
    "The linear regression model is represented as:\n",
    "$$\n",
    "\\hat{y} = X\\theta\n",
    "$$\n",
    "where:\n",
    "- $ X $ is the design matrix with dimensions $ n \\times (m+1) $ (including the intercept term).\n",
    "- $ \\theta$ is the parameter vector with dimensions $ (m+1) \\times 1 $.\n",
    "\n",
    "#### 1.2 Loss Function:\n",
    "The mean squared error (MSE) is given by:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}(X\\theta - y)^T(X\\theta - y)\n",
    "$$\n",
    "\n",
    "#### 1.3 Objective:\n",
    "Given a dataset with $n$ samples and $m$ features, we want to find the parameter vector $\\theta$ that minimizes the mean squared error (MSE) between predicted values $\\hat{y}$ and actual values $y$.\n",
    "$$\n",
    "\\theta = \\arg\\min_\\theta{J(\\theta)}=\\arg\\min_\\theta{(X\\theta - y)^T(X\\theta - y)}\n",
    "$$\n",
    "\n",
    "### 2. Maximum Likelihood Estimation Perspective\n",
    "\n",
    "\n",
    "#### 2.1 Model Representation:\n",
    "The linear regression model is represented as:\n",
    "$$\n",
    "y = X\\theta + \\epsilon\n",
    "$$\n",
    "where $ \\epsilon $ is the error term assumed to be normally distributed with mean zero and variance $ \\sigma^2 $.\n",
    "\n",
    "#### 2.2 Likelihood Function:\n",
    "Under the assumption of Gaussian errors, the likelihood function of the observed data is given by the product of the probability density functions of the individual observations:\n",
    "$$\n",
    "\\mathcal{L}(\\theta | X, y) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "#### 2.3 Log-Likelihood Function:\n",
    "Taking the logarithm of the likelihood function yields the log-likelihood function:\n",
    "$$\n",
    "\\ell(\\theta | X, y) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - X^{(i)}\\theta)^2\n",
    "$$\n",
    "\n",
    "#### 2.4 Objective:\n",
    "We seek to find the parameter vector \\( \\theta \\) that maximizes the likelihood function of the observed data.\n",
    "\n",
    "#### Maximum Likelihood Estimation:\n",
    "To find the maximum likelihood estimate of \\( \\theta \\), we take the derivative of the log-likelihood function with respect to \\( \\theta \\) and set it to zero:\n",
    "$$\n",
    "\\nabla_{\\theta} \\ell(\\theta | X, y) = 0\n",
    "$$\n",
    "Solving this equation analytically yields the maximum likelihood estimate of \\( \\theta \\).\n",
    "\n",
    "The above explanations provide insights into the closed-form solution for linear regression from both the ordinary least squares perspective and the maximum likelihood estimation perspective.\n",
    "\n",
    "### 3 Derivation Steps:\n",
    "1. **Compute the Gradient:**\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{2}{n}X^T(X\\theta - y)\n",
    "$$\n",
    "\n",
    "2. **Set the Gradient to Zero:**\n",
    "$$\n",
    "\\frac{2}{n}X^T(X\\theta - y) = 0\n",
    "$$\n",
    "\n",
    "3. **Solve for \\( \\theta \\):**\n",
    "$$\n",
    "X^TX\\theta - X^Ty = 0\n",
    "$$\n",
    "\n",
    "4. **Obtain the Analytical Solution:**\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bb0d1-e07a-48a4-897b-7f0a5b591b97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Derivation of Analytical Solution for Linear Regression\n",
    "\n",
    "#### Objective:\n",
    "Given a dataset with $n$ samples and $m$ features, we want to find the parameter vector $\\theta$ that minimizes the mean squared error (MSE) between predicted values $\\hat{y}$ and actual values $y$.\n",
    "\n",
    "\n",
    "#### Model Representation:\n",
    "We represent the linear regression model as:\n",
    "$$\n",
    "\\hat{y} = X\\theta\n",
    "$$\n",
    "where:\n",
    "- $ X $ is the design matrix with dimensions $ n \\times (m+1) $ (including the intercept term).\n",
    "- $ \\theta$ is the parameter vector with dimensions $ (m+1) \\times 1 $.\n",
    "\n",
    "#### Loss Function:\n",
    "The mean squared error (MSE) is given by:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}(X\\theta - y)^T(X\\theta - y)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09469bf-2320-4ccc-b12e-5938564d94c5",
   "metadata": {},
   "source": [
    "Sure, here's how you can express the analytical solution for linear regression from the maximum likelihood perspective using Markdown:\n",
    "\n",
    "---\n",
    "\n",
    "### Analytical Solution for Linear Regression from Maximum Likelihood Perspective\n",
    "\n",
    "#### Objective:\n",
    "Linear regression aims to model the relationship between independent variables \\(X\\) and a dependent variable \\(y\\) by fitting a linear equation to observed data. From a maximum likelihood perspective, we seek to maximize the likelihood function to estimate the parameters of the linear model.\n",
    "\n",
    "#### Model Representation:\n",
    "The linear regression model can be represented as:\n",
    "$$\n",
    "y = X\\theta + \\epsilon\n",
    "$$\n",
    "where:\n",
    "- \\(X\\) is the design matrix of size \\(n \\times (m+1)\\), with \\(n\\) observations and \\(m\\) features (plus the intercept term).\n",
    "- \\(\\theta\\) is the parameter vector of size \\((m+1) \\times 1\\) containing the coefficients.\n",
    "- \\(\\epsilon\\) is the error term assumed to be Gaussian noise with mean \\(0\\) and variance \\(\\sigma^2\\).\n",
    "\n",
    "#### Likelihood Function:\n",
    "Under the assumption of Gaussian noise, the likelihood function for linear regression is given by the product of the probability density functions of the observed data:\n",
    "$$\n",
    "\\mathcal{L}(\\theta | X, y) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "#### Log-Likelihood Function:\n",
    "Taking the logarithm of the likelihood function, we obtain the log-likelihood function:\n",
    "$$\n",
    "\\ell(\\theta | X, y) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - X^{(i)}\\theta)^2\n",
    "$$\n",
    "\n",
    "#### Maximum Likelihood Estimation:\n",
    "To find the maximum likelihood estimate of the parameter vector \\( \\theta \\), we maximize the log-likelihood function by taking its derivative with respect to \\( \\theta \\) and setting it to zero:\n",
    "$$\n",
    "\\nabla_{\\theta} \\ell(\\theta | X, y) = 0\n",
    "$$\n",
    "Solving this equation analytically yields the maximum likelihood estimate of \\( \\theta \\).\n",
    "\n",
    "#### Conclusion:\n",
    "The maximum likelihood estimation provides a statistical perspective for finding the optimal parameter vector \\( \\theta \\) in linear regression by maximizing the likelihood function of the observed data.\n",
    "\n",
    "---\n",
    "\n",
    "This markdown provides an explanation of linear regression from the maximum likelihood perspective, including the likelihood function, log-likelihood function, and the process of maximum likelihood estimation for finding the optimal parameter vector \\( \\theta \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b80f7b3-9532-41a6-82a0-b8538f910803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000, method='closed_form', reg=None, alpha=0.01):\n",
    "        self.lr = lr  # 学习率\n",
    "        self.epochs = epochs  # 迭代次数\n",
    "        self.method = method  # 方法：'closed_form'闭式解，'gradient_descent'梯度下降\n",
    "        self.theta = None  # 参数\n",
    "        self.reg = reg  # 正则化方法：None（无罚项），'l1'（L1正则），'l2'（L2正则）\n",
    "        self.alpha = alpha  # 正则化参数\n",
    "        self.loss_history = [] # Loss record\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 添加偏置项\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        if self.method == 'closed_form':\n",
    "            # 闭式解\n",
    "            if self.reg is None:\n",
    "                # 无正则化\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "            elif self.reg == 'l1':\n",
    "                # L1正则化（Lasso）\n",
    "                L = np.eye(X_b.shape[1])\n",
    "                L[0, 0] = 0  # 不对偏置项进行惩罚\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b) + self.alpha * L).dot(X_b.T).dot(y)\n",
    "            elif self.reg == 'l2':\n",
    "                # L2正则化（Ridge）\n",
    "                self.theta = np.linalg.inv(X_b.T.dot(X_b) + self.alpha * np.eye(X_b.shape[1])).dot(X_b.T).dot(y)\n",
    "        elif self.method == 'gradient_descent':\n",
    "            # 初始化参数\n",
    "            self.theta = np.random.randn(X_b.shape[1], 1)\n",
    "            # 梯度下降\n",
    "            for _ in range(self.epochs):\n",
    "                gradients = 2 / len(X_b) * X_b.T.dot(X_b.dot(self.theta) - y)\n",
    "                if self.reg == 'l1':\n",
    "                    # L1正则化（Lasso）\n",
    "                    gradients[1:] += self.alpha * np.sign(self.theta[1:])\n",
    "                elif self.reg == 'l2':\n",
    "                    # L2正则化（Ridge）\n",
    "                    gradients[1:] += 2 * self.alpha * self.theta[1:]\n",
    "                self.theta -= self.lr * gradients\n",
    "                loss = np.mean((X_b.dot(self.theta) - y) ** 2)\n",
    "                self.loss_history.append(loss)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method. Choose 'closed_form' or 'gradient_descent'.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 添加偏置项\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5361ec-405e-4f3b-8a3b-b0bfaaff2114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "# Introduce outliers\n",
    "# X = np.linspace(0,100,100)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_outliers = np.array([[1], [1.01], [1.02],[1.03], [1.04], [1.05]])\n",
    "y_outliers = np.array([[30], [35], [39],[40], [41], [42]])\n",
    "X = np.vstack((X, X_outliers))\n",
    "y = np.vstack((y, y_outliers))\n",
    "methods = ['closed_form','gradient_descent']\n",
    "regs = [None,'l1','l2']\n",
    "# Instantiate and fit the models\n",
    "# No penalty (ordinary linear regression)\n",
    "models = {}\n",
    "for m in methods:\n",
    "    for r in regs:\n",
    "        models['{}_{}'.format(m,r)] = LinearRegression(method=m,reg=r,alpha=10)\n",
    "        models['{}_{}'.format(m,r)].fit(X,y)\n",
    "w, b = [], []\n",
    "for k in models:\n",
    "    p = models[k].theta.ravel()\n",
    "    w.append(p[0])\n",
    "    b.append(p[1])\n",
    "df = pd.DataFrame({'solution':list(models.keys()),'w':w,'b':b})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a6252-6f75-47ec-90e4-0dc8e4d816dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize data and fitted models\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Scatter plot of original data\n",
    "plt.scatter(X, y, color='blue', label='Original data')\n",
    "\n",
    "# Fitted lines for closed-form solution and gradient descent\n",
    "for m in methods:\n",
    "    for r in regs:\n",
    "        plt.plot(X, models['{}_{}'.format(m,r)].predict(X), label='{}_{}'.format(m,r))\n",
    "# plt.plot(X, lin_reg_no_penalty_gd.predict(X), color='red', label='Gradient descent')\n",
    "\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4671845-8188-46d5-8b72-8dd530a0de23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot loss change during gradient descent\n",
    "plt.figure(figsize=(8, 5))\n",
    "for r in regs:\n",
    "    # plt.plot(X, models['{}_{}'.format(m,r)].predict(X), label='{}_{}'.format(m,r))\n",
    "    plt.plot(range(len(models['{}_{}'.format(methods[1],r)].loss_history)), \n",
    "             models['{}_{}'.format(methods[1],r)].loss_history, \n",
    "             label='{}_{}'.format(methods[1],r))\n",
    "plt.title('Gradient Descent Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
